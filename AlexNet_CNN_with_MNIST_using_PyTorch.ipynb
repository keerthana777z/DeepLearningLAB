{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pndi_GzVvB2",
        "outputId": "ab3ff0e5-966e-4dc0-f9da-0e83c640e40b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 3, 112, 112])\n",
            "After Conv1: torch.Size([1, 64, 56, 56])\n",
            "After ReLU1: torch.Size([1, 64, 56, 56])\n",
            "After MaxPool1: torch.Size([1, 64, 27, 27])\n",
            "After Conv2: torch.Size([1, 192, 27, 27])\n",
            "After ReLU2: torch.Size([1, 192, 27, 27])\n",
            "After MaxPool2: torch.Size([1, 192, 13, 13])\n",
            "After Conv3: torch.Size([1, 384, 13, 13])\n",
            "After ReLU3: torch.Size([1, 384, 13, 13])\n",
            "After Conv4: torch.Size([1, 256, 13, 13])\n",
            "After ReLU4: torch.Size([1, 256, 13, 13])\n",
            "After Conv5: torch.Size([1, 256, 13, 13])\n",
            "After ReLU5: torch.Size([1, 256, 13, 13])\n",
            "After MaxPool3: torch.Size([1, 256, 6, 6])\n",
            "After Flatten: torch.Size([1, 9216])\n",
            "After Linear1: torch.Size([1, 4096])\n",
            "After ReLU6: torch.Size([1, 4096])\n",
            "After Linear2: torch.Size([1, 4096])\n",
            "After ReLU7: torch.Size([1, 4096])\n",
            "After Linear3: torch.Size([1, 1000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define AlexNet class with updated padding, stride, and input size\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Conv1 Layer: 3 input channels, 64 output channels, 7x7 kernel, stride 2, padding 3\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "            # Conv2 Layer: 64 input channels, 192 output channels, 5x5 kernel, padding 2\n",
        "            nn.Conv2d(64, 192, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "\n",
        "            # Conv3 Layer: 192 input channels, 384 output channels, 3x3 kernel, padding 1\n",
        "            nn.Conv2d(192, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Conv4 Layer: 384 input channels, 256 output channels, 3x3 kernel, padding 1\n",
        "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # Conv5 Layer: 256 input channels, 256 output channels, 3x3 kernel, padding 1\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256 * 6 * 6, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(\"Input shape:\", x.shape)\n",
        "\n",
        "        x = self.features[0](x)  # Conv1\n",
        "        print(\"After Conv1:\", x.shape)\n",
        "        x = self.features[1](x)  # ReLU1\n",
        "        print(\"After ReLU1:\", x.shape)\n",
        "        x = self.features[2](x)  # MaxPool1\n",
        "        print(\"After MaxPool1:\", x.shape)\n",
        "\n",
        "        x = self.features[3](x)  # Conv2\n",
        "        print(\"After Conv2:\", x.shape)\n",
        "        x = self.features[4](x)  # ReLU2\n",
        "        print(\"After ReLU2:\", x.shape)\n",
        "        x = self.features[5](x)  # MaxPool2\n",
        "        print(\"After MaxPool2:\", x.shape)\n",
        "\n",
        "        x = self.features[6](x)  # Conv3\n",
        "        print(\"After Conv3:\", x.shape)\n",
        "        x = self.features[7](x)  # ReLU3\n",
        "        print(\"After ReLU3:\", x.shape)\n",
        "\n",
        "        x = self.features[8](x)  # Conv4\n",
        "        print(\"After Conv4:\", x.shape)\n",
        "        x = self.features[9](x)  # ReLU4\n",
        "        print(\"After ReLU4:\", x.shape)\n",
        "\n",
        "        x = self.features[10](x)  # Conv5\n",
        "        print(\"After Conv5:\", x.shape)\n",
        "        x = self.features[11](x)  # ReLU5\n",
        "        print(\"After ReLU5:\", x.shape)\n",
        "        x = self.features[12](x)  # MaxPool3\n",
        "        print(\"After MaxPool3:\", x.shape)\n",
        "\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        print(\"After Flatten:\", x.shape)\n",
        "\n",
        "        x = self.classifier[0](x)  # Dropout\n",
        "        x = self.classifier[1](x)  # Linear1\n",
        "        print(\"After Linear1:\", x.shape)\n",
        "        x = self.classifier[2](x)  # ReLU6\n",
        "        print(\"After ReLU6:\", x.shape)\n",
        "        x = self.classifier[3](x)  # Dropout2\n",
        "        x = self.classifier[4](x)  # Linear2\n",
        "        print(\"After Linear2:\", x.shape)\n",
        "        x = self.classifier[5](x)  # ReLU7\n",
        "        print(\"After ReLU7:\", x.shape)\n",
        "        x = self.classifier[6](x)  # Linear3\n",
        "        print(\"After Linear3:\", x.shape)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Example of loading the AlexNet model and using it for inference\n",
        "model = AlexNet(num_classes=1000)\n",
        "\n",
        "# Example of using the model with an input tensor (batch size of 1, 3 channels, 112x112 image)\n",
        "input_tensor = torch.randn(1, 3, 112, 112)  # Random tensor with size (batch, channels, height, width)\n",
        "output = model(input_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# Define the optimized AlexNet class\n",
        "class OptimizedAlexNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(OptimizedAlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Fewer filters\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.dummy_input = torch.zeros(1, 1, 64, 64)\n",
        "        self.flattened_size = self._get_flattened_size(self.dummy_input)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.flattened_size, 512),  # Smaller fully connected layers\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    def _get_flattened_size(self, x):\n",
        "        x = self.features(x)\n",
        "        return x.view(x.size(0), -1).size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Data preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Smaller size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "# Load the MNIST dataset\n",
        "trainset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)  # Smaller batch size\n",
        "\n",
        "testset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = OptimizedAlexNet(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 2  # Fewer epochs for quicker training\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 50 == 49:  # Print every 50 batches\n",
        "            print(f\"Epoch [{epoch+1}/{epochs}], Batch [{i+1}], Loss: {running_loss/50:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"Training Finished\")\n",
        "\n",
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Accuracy on the test images: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i7GHwZlRExw",
        "outputId": "a6439534-1986-4d6f-f3fa-ec4cd6a70b92"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/2], Batch [50], Loss: 1.1152\n",
            "Epoch [1/2], Batch [100], Loss: 0.3624\n",
            "Epoch [1/2], Batch [150], Loss: 0.2302\n",
            "Epoch [1/2], Batch [200], Loss: 0.1765\n",
            "Epoch [1/2], Batch [250], Loss: 0.1845\n",
            "Epoch [1/2], Batch [300], Loss: 0.1417\n",
            "Epoch [1/2], Batch [350], Loss: 0.1390\n",
            "Epoch [1/2], Batch [400], Loss: 0.1093\n",
            "Epoch [1/2], Batch [450], Loss: 0.1353\n",
            "Epoch [1/2], Batch [500], Loss: 0.0922\n",
            "Epoch [1/2], Batch [550], Loss: 0.1171\n",
            "Epoch [1/2], Batch [600], Loss: 0.1006\n",
            "Epoch [1/2], Batch [650], Loss: 0.0987\n",
            "Epoch [1/2], Batch [700], Loss: 0.0706\n",
            "Epoch [1/2], Batch [750], Loss: 0.0874\n",
            "Epoch [1/2], Batch [800], Loss: 0.0929\n",
            "Epoch [1/2], Batch [850], Loss: 0.0968\n",
            "Epoch [1/2], Batch [900], Loss: 0.0709\n",
            "Epoch [1/2], Batch [950], Loss: 0.0807\n",
            "Epoch [1/2], Batch [1000], Loss: 0.0667\n",
            "Epoch [1/2], Batch [1050], Loss: 0.0729\n",
            "Epoch [1/2], Batch [1100], Loss: 0.0726\n",
            "Epoch [1/2], Batch [1150], Loss: 0.0488\n",
            "Epoch [1/2], Batch [1200], Loss: 0.0700\n",
            "Epoch [1/2], Batch [1250], Loss: 0.0621\n",
            "Epoch [1/2], Batch [1300], Loss: 0.0740\n",
            "Epoch [1/2], Batch [1350], Loss: 0.0580\n",
            "Epoch [1/2], Batch [1400], Loss: 0.0697\n",
            "Epoch [1/2], Batch [1450], Loss: 0.0514\n",
            "Epoch [1/2], Batch [1500], Loss: 0.0668\n",
            "Epoch [1/2], Batch [1550], Loss: 0.0736\n",
            "Epoch [1/2], Batch [1600], Loss: 0.0499\n",
            "Epoch [1/2], Batch [1650], Loss: 0.0679\n",
            "Epoch [1/2], Batch [1700], Loss: 0.0600\n",
            "Epoch [1/2], Batch [1750], Loss: 0.0627\n",
            "Epoch [1/2], Batch [1800], Loss: 0.0374\n",
            "Epoch [1/2], Batch [1850], Loss: 0.0493\n",
            "Epoch [2/2], Batch [50], Loss: 0.0268\n",
            "Epoch [2/2], Batch [100], Loss: 0.0432\n",
            "Epoch [2/2], Batch [150], Loss: 0.0236\n",
            "Epoch [2/2], Batch [200], Loss: 0.0482\n",
            "Epoch [2/2], Batch [250], Loss: 0.0313\n",
            "Epoch [2/2], Batch [300], Loss: 0.0335\n",
            "Epoch [2/2], Batch [350], Loss: 0.0425\n",
            "Epoch [2/2], Batch [400], Loss: 0.0457\n",
            "Epoch [2/2], Batch [450], Loss: 0.0420\n",
            "Epoch [2/2], Batch [500], Loss: 0.0300\n",
            "Epoch [2/2], Batch [550], Loss: 0.0334\n",
            "Epoch [2/2], Batch [600], Loss: 0.0351\n",
            "Epoch [2/2], Batch [650], Loss: 0.0673\n",
            "Epoch [2/2], Batch [700], Loss: 0.0410\n",
            "Epoch [2/2], Batch [750], Loss: 0.0231\n",
            "Epoch [2/2], Batch [800], Loss: 0.0370\n",
            "Epoch [2/2], Batch [850], Loss: 0.0405\n",
            "Epoch [2/2], Batch [900], Loss: 0.0561\n",
            "Epoch [2/2], Batch [950], Loss: 0.0444\n",
            "Epoch [2/2], Batch [1000], Loss: 0.0331\n",
            "Epoch [2/2], Batch [1050], Loss: 0.0505\n",
            "Epoch [2/2], Batch [1100], Loss: 0.0418\n",
            "Epoch [2/2], Batch [1150], Loss: 0.0528\n",
            "Epoch [2/2], Batch [1200], Loss: 0.0336\n",
            "Epoch [2/2], Batch [1250], Loss: 0.0420\n",
            "Epoch [2/2], Batch [1300], Loss: 0.0324\n",
            "Epoch [2/2], Batch [1350], Loss: 0.0185\n",
            "Epoch [2/2], Batch [1400], Loss: 0.0537\n",
            "Epoch [2/2], Batch [1450], Loss: 0.0403\n",
            "Epoch [2/2], Batch [1500], Loss: 0.0314\n",
            "Epoch [2/2], Batch [1550], Loss: 0.0347\n",
            "Epoch [2/2], Batch [1600], Loss: 0.0282\n",
            "Epoch [2/2], Batch [1650], Loss: 0.0384\n",
            "Epoch [2/2], Batch [1700], Loss: 0.0388\n",
            "Epoch [2/2], Batch [1750], Loss: 0.0345\n",
            "Epoch [2/2], Batch [1800], Loss: 0.0415\n",
            "Epoch [2/2], Batch [1850], Loss: 0.0380\n",
            "Training Finished\n",
            "Accuracy on the test images: 98.75%\n"
          ]
        }
      ]
    }
  ]
}